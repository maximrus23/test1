{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKJWSvSKOl1c"
   },
   "source": [
    "# PySpark Demo and Word Counting with Spark\n",
    "\n",
    "To get you started, we'll walk you through a bit of Colab specific Python and some PySpark code, and then we'll do the classic word count example, followd by some tasks for you to try.\n",
    "\n",
    "**Please run through the notebook cell by cell (using 'run' above or 'shift-return' on the keyboard).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95B3FYvqPya6"
   },
   "source": [
    "##Preliminaries: Preparing Colab and Spark\n",
    "1.   When you open this notebook from the shared \"Data-Engineering\" folder, you don't have write acceess. When you save it, a copy will be created in the folder \"Colab Notebooks\".\n",
    "2.   The code below will mount Google Drive as a directory in the file system (the machine is a virtual Linux box). You will be asked to authorise this and provide an authentication code available through a link.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yVdG2kvLokM"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.3.3-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "8edGFcfkPx50",
    "outputId": "7b2fe436-1436-49c1-c03e-fa442d09e545"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the Drive helper and mount\n",
    "#from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "## just added this comment and imported numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yD0VHJ-nQlVS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.3-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9mm1hVyQdZe"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpO-PaoGUID4"
   },
   "source": [
    "Next we move to the \"Colab Notebooks\" folder on your drive and create subfolder \"data\". Then we copy the \"hamlet.txt\" file there (you can check on Google Drive if it has worked). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "_Bn28SFjUHC_",
    "outputId": "506f690c-35c9-44bb-dbc0-ebd175893c77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive\n",
      "mkdir: cannot create directory ‘Colab Notebooks’: File exists\n",
      "/content/drive/My Drive/Colab Notebooks\n",
      "mkdir: cannot create directory ‘data’: File exists\n",
      "/content/drive/My Drive/Colab Notebooks/data\n",
      "total 78946\n",
      "-rw------- 1 root root   193083 Jan 22 08:56 hamlet.txt\n",
      "-rw------- 1 root root 80208848 Jan 22 08:55 trip.csv\n",
      "-rw------- 1 root root   438063 Jan 22 08:55 weather.csv\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My\\ Drive/\n",
    "!mkdir \"Colab Notebooks\"\n",
    "%cd \"Colab Notebooks\"\n",
    "!mkdir data\n",
    "%cd data\n",
    "!cp \"/content/drive/My Drive/Data-Engineering-Workshop-Week-2/data/hamlet.txt\" .\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpUd_fHJRInX"
   },
   "source": [
    "Next, we Install Spark (may take a while) and altair for visualisaion. This will need to be done every time a new machine is created. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "2S9ShIHjSlDS",
    "outputId": "545cd11a-10f7-4e8b-ccf1-4f8f1f989522"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n",
      "Requirement already satisfied: altair in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair) (0.10.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair) (2.6.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from altair) (0.25.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from altair) (2.10.3)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair) (0.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from altair) (1.17.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->altair) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->altair) (2.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->altair) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->altair) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lVuFYFEmjWjx"
   },
   "source": [
    "If the installation above doesnn't work, try the one below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rmJvtPxjOl1e"
   },
   "source": [
    "## Part 1 - Demo: Apapche Spark API with PySpark\n",
    "\n",
    "Basically there are 32APIs available in Apache Spark - RDD (Resilient Distributed Datasets) and DataFrame (extended by Dataset in Scala and Java). In this lab we will look at RDDs and Dataframes in Python.\n",
    "\n",
    "For more information on the Spark framework - visit (https://spark.apache.org)\n",
    "For more information on the Pyspark API - visit (https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmoNsdqQOl1f"
   },
   "source": [
    "### 1) Access to Spark\n",
    "\n",
    "We start by cretaing a SparkContext, normally called `sc`. \n",
    "We use that to create RDDs and a SparkSession object (for DataFrames), often just called `spark`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "12aW4ncAOl1h",
    "outputId": "a18b8511-f29c-4fa2-fb90-44219d900f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f10dd61e320>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "# get a spark context\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "# get the context\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctb3qeAkOl1m"
   },
   "source": [
    "### 2) RDD Creation\n",
    "\n",
    "There are two ways to create RDDs. The first is to parallelise a Python object that exists in your driver process (i.e. this one). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbuuRZv6Ol1n"
   },
   "source": [
    "The second way is to create an RDD is by referencing an external dataset such as a shared filesystem, HDFS, HBase, or just data source offering a Hadoop InputFormat. This is what we will be using in this lab (further down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "phwtMBs2Ol1p",
    "outputId": "0acd341c-b2fd-4952-8385-e02d960ddb67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195\n"
     ]
    }
   ],
   "source": [
    "# Creat an RDD from a Python object in this process (the \"driver\").\n",
    "# The parallelize function  creating the \"numbers\" RDD\n",
    "data = [1,2,3,4,5]\n",
    "firstRDD = sc.parallelize(data)\n",
    "print(firstRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx75RI9mOl1s"
   },
   "source": [
    "This RDD lives now on as many worker machines as are available and as are deemed useful by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDnZCdSTOl1t"
   },
   "source": [
    "### 3) RDD operations\n",
    "RDDs have two kinds of operations: *Transformations* and *Actions*.\n",
    "\n",
    "*Transformations* create a new RDD by applying a function to the items in the RDD. The function will be remembered, but only be applied when needed (\"*lazy* evaluation\").\n",
    "\n",
    "*Actions* produce some output from the data. An *Action* will trigger the execution of all *Transformations*.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Q25kDkt9Ol1v",
    "outputId": "9f59547b-ff2d-4f87-ff45-5ea2b73ac330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def myfun(x):\n",
    "  return x+3\n",
    "# lambda function: x -> x+3\n",
    "#RDD2 = firstRDD.map(lambda x:x+3)  \n",
    "RDD2 = firstRDD.map(myfun)  \n",
    "print(RDD2)\n",
    "# nothing happened to far, as there is no action\n",
    "print(firstRDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhIIytM9bSvm"
   },
   "source": [
    "If the functions are short (one expression, to be exact), this is more convenient write with a lamba expression, that creates an anonymous function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IEtzNnm4bZfe",
    "outputId": "bd49a4fc-756d-4f17-8c62-7093ab772306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[2] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    " RDD3 = firstRDD.map(lambda x:x+3) # this is the same as using myfun \n",
    "print(RDD3)\n",
    "# nothing happened to far, as there is no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ju4PvD5gOl10",
    "outputId": "1d81b40d-50ce-4bbf-e413-21b73072a48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# \"count\" is an action and triggers the transformation   \n",
    "a = RDD2.countApprox(10000, 0.95) \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xp2aTh8Ol15"
   },
   "source": [
    "`collect` is an action that returns the values of the RDD in an Python array, back into this local driver process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tdaNRc0SOl15",
    "outputId": "686281b0-0774-405c-bc4c-8147ce9e5e9d"
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f74afc987cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRDD2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this resulted from mapping with the named function 'myfun'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Unsupported class file major version 55'"
     ]
    }
   ],
   "source": [
    "a = RDD2.collect() # this resulted from mapping with the named function 'myfun'\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXmtXG0hOl19"
   },
   "source": [
    "As we can seee above, *myfun* (RDD2) and the *lambda x: x+3* (RDD3) have the same effect.\n",
    "\n",
    "Look here for more information about the functions provided by the RDD class: (https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUl3upk0Ol1-"
   },
   "source": [
    "### 4) Dataframes \n",
    "\n",
    "Dataframes are a more structured form of storage than RDDs and similar to Pandas dataframes.  \n",
    "\n",
    "Let us see how to create and use dataframes. There are three ways of creating a dataframe\n",
    "    a) from an existing RDD.\n",
    "    b) form and external data source, e.g., loading the data from JSON or CSV files.\n",
    "    c) Programmatically specifying schema and data.\n",
    "    \n",
    "Here is an example for option a). We use the *Row* class to create structure data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hSIzFqAQOl1_",
    "outputId": "56648a8a-7c24-4af2-9d0a-825560dd4763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[age: bigint, name: string]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "dataList = [('Anne',21),('Bob',22),('Carl',29),('Daisy',36)] # our data as a list\n",
    "rdd = sc.parallelize(dataList) # RDD from the list\n",
    "peopleRDD = rdd.map(lambda x: Row(name=x[0], age=int(x[1]))) # RDD\n",
    "peopleDF = spark.createDataFrame(peopleRDD) \n",
    "print(peopleDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsGAOxX2Ol2G"
   },
   "source": [
    "## Part 2: Classic Word Count\n",
    "\n",
    "We will now do the classic word count example for the MapReduce pattern.\n",
    "\n",
    "We will apply it to the text of Sheakespeare's play *Hamlet*. For that you should have uploaded the file \"hamlet.txt\" into the data assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eWjGlkbOl2H"
   },
   "source": [
    "### 6) Load the data\n",
    "First we need to load the text into an RDD (the second method of creating an RDD as mentioned above). \n",
    "\n",
    "We need to specify the path, and we can read directly from the shared Data-Engineering directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzdsuhxtOl2H"
   },
   "outputs": [],
   "source": [
    "filepath = \"/content/drive/My Drive/Data-Engineering/data/hamlet.txt\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5q8UiCtOl2J"
   },
   "source": [
    "You can read the file into an RDD with `textFile`. The RDD then contains as items the lines of the text. `take(3)` then gives us the first 3 lines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "OM_vQDsGOl2J",
    "outputId": "5ef478de-2569-437a-ff4d-37f905b94df1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project Gutenberg Etext of Hamlet by Shakespeare',\n",
       " \"PG has multiple editions of William Shakespeare's Complete Works\",\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineRDD = sc.textFile(filepath)\n",
    "lineRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ab6vKwgvOl2L"
   },
   "source": [
    "### 7) Split lines into words\n",
    "\n",
    "In order to count the words, we need to split the lines into words. We can do that using the `split` function of the Python String class to separate at each space. \n",
    "\n",
    "The map function replaces each item with a new one, in this case our `lambda` returns an array of words (provided by `split(' ')`). However, we want to create one item per word, therefore we need to use a function called `flatMap` that creates a new RDD item for every item in the array returned by the lambda.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WwI2cDlSOl2L",
    "outputId": "9a050cdf-a8a2-472a-8d77-15f3f286b912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project', 'Gutenberg', 'Etext']"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRDD = lineRDD.flatMap(lambda x: x.split(' '))\n",
    "wordRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDuHh_ocOl2N"
   },
   "source": [
    "Map the words to tuples of the form *(word, 1)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0PXZXAGpOl2O",
    "outputId": "64e707e9-3960-4ca7-da86-57a15db08b3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 1), ('Gutenberg', 1), ('Etext', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1RDD = wordRDD.map(lambda x: (x, 1))\n",
    "word1RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BplN07GOl2P"
   },
   "source": [
    "### 8) Count by reducing\n",
    "For Spark, the first part in each tuple is the 'key'. Now we can use reduceByKey() to add the 1s and get the number of occurences per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HRFaQARsOl2Q",
    "outputId": "ef0c282a-892c-4fc6-8151-3b6a66ece343"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 21), ('Etext', 4), ('of', 679)]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n",
    "wordCountRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G38rUrlVOl2S"
   },
   "source": [
    "### 9) Filtering \n",
    "\n",
    "There are many empty strings returned by the splitting. We can remove them by filtering.\n",
    "Then can take a shortcut and use a ready-made functions 'count by value', which does the same as we before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZQ9mBefpOl2S",
    "outputId": "e9a7a75e-1a58-4b64-bdcc-ce0f9c893584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Project', 21), ('Etext', 4), ('of', 679), ('Shakespeare', 5), ('multiple', 3)]\n"
     ]
    }
   ],
   "source": [
    "wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0)\n",
    "word1RDD = wordFilteredRDD.map(lambda x: (x, 1))\n",
    "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n",
    "wcList = wordCountRDD.take(5)\n",
    "print(wcList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyubLNalOl2W"
   },
   "source": [
    "## Part 3: Tasks for you to work on\n",
    "\n",
    "Based on the examples above, you can now try and write some code yourself.  Look for the lines starting with **>>>**. You neeed to fix them by writing your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LY4f9UwOl2W"
   },
   "source": [
    "## Task 1) Better splitting \n",
    "\n",
    "Currently our 'words' can contain punctuation, becausee only spaces are removed. A better way to split is using regular expressions  (Python's 're' package)(https://docs.python.org/3.5/library/re.html?highlight=regular%20expressions). `re.split('\\W+', 'my. test. string!')` does a good job. Try it out below by fixing the line that starts with '>>>'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wuQDbhOAOl2X",
    "outputId": "c0df080e-7b9f-4a8d-bcab-3c8b146729fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project', 'Gutenberg', 'Etext']"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "wordFilteredRDD = wordRDD.filter(lambda x: ...) # filtering\n",
    "wordFilteredRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktBnkI5TOl2Y"
   },
   "source": [
    "## 2) Use lower case\n",
    "\n",
    "Convert all strings to lower case (using `.lower()` provided by the Python string class), so that 'Test' and 'test' count as the same. Package it into one a tuple of the form (word,1) in the same call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DBqa2EQjOl2Y",
    "outputId": "02e60287-e097-4a7a-b1f6-091417b130e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project', 'gutenberg', 'etext']"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> wordLowerRDD = wordFilteredRDD.map(lambda x: ..)\n",
    "wordLowerRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "WIVwV56lOl2a",
    "outputId": "d36aa2c5-75db-473d-b18c-c7ba6b2c7ace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 27),\n",
       " ('gutenberg', 16),\n",
       " ('of', 727),\n",
       " ('shakespeare', 5),\n",
       " ('multiple', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1RDD = wordLowerRDD.map(lambda x: (x,1)) # we can now get better word count results\n",
    "wordCountRDD = word1RDD.reduceByKey(lambda ...) # we can now get better word count results\n",
    "wordCountRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7jgnYSVOl2b"
   },
   "source": [
    "## 3) Filter rare words\n",
    "\n",
    "Add a filtering step call remove all words with less than 5 occurrences. This can be useful to identify common topics in documents, where very rare words can be misleading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "CTd9S_y6Ol2b",
    "outputId": "3969ec20-e4ec-45d2-edac-cea5f10d1aa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 27),\n",
       " ('gutenberg', 16),\n",
       " ('of', 727),\n",
       " ('shakespeare', 5),\n",
       " ('multiple', 3)]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the trick here is to apply the lambda only to the second part of each item, i.e. x[1] \n",
    "freqWordsRDD = wordCountRDD.filter(lambda x:  ... ) # tip: filter keeps the times where the lambda returns true.\n",
    "freqWordsRDD.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IypcO9SPOl2d"
   },
   "source": [
    "## 4) List only stopwords\n",
    "\n",
    "Stopwords are frequent words that are not topic-specifc.  Stopwords can be useful for recognising the style of an author. Removing stopwords can be useful in regocnising the topic of a document. \n",
    "\n",
    "Below is a small list of stopwords. Filter the tuples where the first part is a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "T85Nx6ZuOl2e",
    "outputId": "bfe6e63e-637e-440e-f965-4ae5ecb67cad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 27),\n",
       " ('gutenberg', 16),\n",
       " ('of', 727),\n",
       " ('shakespeare', 5),\n",
       " ('multiple', 3),\n",
       " (\"shakespeare's\", 2),\n",
       " ('copyright', 9),\n",
       " ('are', 137),\n",
       " ('world,', 3),\n",
       " ('sure', 6)]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWordList = ['the','a','in','of','on','at','for','by','I','you','me'] \n",
    "stopWordsRDD = freqWordsRDD.filter(lambda x: ...) # the 1st part of the tuple should be in the list\n",
    "stopWordsRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKVS18B9Ol2f"
   },
   "source": [
    "There are only a few words, so we can see the vies results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "JsX57MXHOl2g",
    "outputId": "339783c1-90dc-446c-a17c-a5add7e35ba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project: 27\n",
      "gutenberg: 16\n",
      "of: 727\n",
      "shakespeare: 5\n",
      "multiple: 3\n",
      "shakespeare's: 2\n",
      "copyright: 9\n",
      "are: 137\n",
      "world,: 3\n",
      "sure: 6\n"
     ]
    }
   ],
   "source": [
    "output = stopWordsRDD.take(10) \n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJcV-ulGOl2i"
   },
   "source": [
    "We can now visualise the stopword counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "Y3p___CROl2i",
    "outputId": "1f462a9f-9e44-47ba-9eac-bed7b07aee98",
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "barChart",
      "keyFields": "_1",
      "rowCount": "500",
      "valueFields": "_2"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>project</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gutenberg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shakespeare</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multiple</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shakespeare's</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>copyright</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>are</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>world,</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sure</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words  items\n",
       "0        project     27\n",
       "1      gutenberg     16\n",
       "2             of    727\n",
       "3    shakespeare      5\n",
       "4       multiple      3\n",
       "5  shakespeare's      2\n",
       "6      copyright      9\n",
       "7            are    137\n",
       "8         world,      3\n",
       "9           sure      6"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "word_list=[]\n",
    "item_list=[]\n",
    "for item in output:\n",
    "  (word,count)=item\n",
    "  word_list.append(word)\n",
    "  item_list.append(count)\n",
    "df3=pd.DataFrame({'words':word_list,'items':item_list})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIG2PrxEQNW5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Word_Counting_with_Spark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
